name: Publications Crawler

on:
  push:
    branches: [master, main]
  schedule:
    # Run every Sunday at 2:00 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch: # Allow manual triggering

jobs:
  crawl-publications:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create necessary directories
        run: |
          mkdir -p logs

      - name: Run crawler
        run: |
          python main.py

      - name: Create summary
        run: |
          echo "## Crawling Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Publications crawling completed" >> $GITHUB_STEP_SUMMARY
          echo "- Data sent to API endpoint" >> $GITHUB_STEP_SUMMARY
          echo "- Log file: logs/crawler.log" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run completed at:** $(date)" >> $GITHUB_STEP_SUMMARY
